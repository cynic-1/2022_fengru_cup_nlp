{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "with open('./语料库/语料库/docs/doc_names.txt', 'r', encoding='utf-8', errors='ignore')as fn:\n",
    "    names = fn.read().splitlines()\n",
    "train = []\n",
    "for name in names:\n",
    "    # 将分完词的文档加载成符合gensim文格式的输入\n",
    "    # print(name)\n",
    "    with open('./语料库/语料库/docs/' + name, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f.readlines():\n",
    "            line = [wnl.lemmatize(word.strip().lower()) for word in line.split() if word.lower() not in stopwords.words('english')]\n",
    "            train.append(line)\n",
    "\n",
    "with open('pre_process.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in train:\n",
    "        for it in item:\n",
    "            f.write(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=91446, num_nnz=2011109)\n"
     ]
    }
   ],
   "source": [
    "# 构造词典\n",
    "dictionary = corpora.Dictionary(train)\n",
    "feature_cnt = len(dictionary.token2id)  # 词典中词的数量\n",
    "dictionary.save('dict.txt')  # 保存生成的词典,用于以后加载\n",
    "# dictionary=Dictionary.load('dict.txt')#加载词典\n",
    "\n",
    "# 基于词典，将【分词列表集】转换成【向量集】，形成【语料库】\n",
    "corpus = [dictionary.doc2bow(text) for text in train]\n",
    "\n",
    "# 使用【TF-IDF模型】处理语料库\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "\n",
    "# 打印模型参数：文档数量与语料库单词数\n",
    "print(tfidf_model)\n",
    "\n",
    "# 存储通过tf-idf转化过的文档\n",
    "with open('tfidf_doc.txt', 'w', encoding='utf-8') as fr:\n",
    "    for doc in tfidf_model[corpus]:\n",
    "        fr.write(doc.__str__() + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence = []\n",
    "# 假设主题数量在1-20中进行选择\n",
    "for n in range(1, 21):   \n",
    "    # Multi-core implementation:\n",
    "    lda = gensim.models.LdaMulticore(corpus, num_topics = n, id2word = dictionary, passes = 10, workers = 2)\n",
    "    \n",
    "    # ompute coherence for each lda model with different number of topics\n",
    "    cohm = CoherenceModel(model = lda, corpus = corpus, dictionary = dictionary, coherence = 'u_mass')\n",
    "    coh = cohm.get_coherence()\n",
    "    coherence.append(coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)\n",
    "topic = [i for i in range(1,21)]\n",
    "plt.plot(topic,coherence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=153269, num_topics=5, decay=0.5, chunksize=1000)\n"
     ]
    }
   ],
   "source": [
    "# 利用lda模型对语料库进行建模，设置模型的主题的个数、迭代次数\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, chunksize=1000, iterations=2000)\n",
    "# 打印LDA模型参数\n",
    "print(lda)\n",
    "\n",
    "# 存储LDA文档结果：存储文档-主题分布\n",
    "with open('doc_dis.txt', 'w') as f1:\n",
    "    # for doc in lda_corpus:\n",
    "    for doc in lda[corpus]:\n",
    "        f1.write(doc.__str__() + '\\n')\n",
    "        f1.flush()\n",
    "        \n",
    "# 存储LDA文档结果：存储主题-词分布\n",
    "with open('topic_dis.txt', 'w') as f2:\n",
    "    for topic in lda.get_topics():\n",
    "        f2.write(topic.__str__() + '\\n')\n",
    "        f2.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.039*\"zte\" + 0.019*\"u\" + 0.017*\"ban\" + 0.015*\"deal\" + 0.012*\"company\" + 0.012*\"bill\" + 0.012*\"lawmaker\" + 0.012*\"senate\" + 0.011*\"department\" + 0.011*\"chinese\" + 0.010*\"trump\" + 0.010*\"billion\" + 0.010*\"said\" + 0.010*\"play\" + 0.009*\"commerce\" + 0.009*\"25%\" + 0.008*\"defense\" + 0.008*\"company\\'s\" + 0.008*\"share\" + 0.008*\"want\" + 0.008*\"lost\" + 0.008*\"first\" + 0.008*\"still\" + 0.008*\"trading\" + 0.008*\"cornyn\" + 0.008*\"resumed\" + 0.008*\"measure\" + 0.007*\"president\" + 0.007*\"last\" + 0.007*\"hong\"')\n",
      "(1, '0.006*\"wrong?\" + 0.006*\"play\" + 0.006*\"actually\" + 0.005*\"zte\" + 0.005*\"u\" + 0.005*\"deal\" + 0.005*\"billion\" + 0.005*\"ban\" + 0.005*\"first\" + 0.005*\"chinese\" + 0.005*\"said\" + 0.005*\"company\" + 0.005*\"lost\" + 0.005*\"department\" + 0.005*\"president\" + 0.005*\"bill\" + 0.005*\"senate\" + 0.005*\"still\" + 0.005*\"measure\" + 0.005*\"trump\" + 0.005*\"lawmaker\" + 0.005*\"share\" + 0.005*\"last\" + 0.005*\"cornyn\" + 0.005*\"company\\'s\" + 0.005*\"hong\" + 0.005*\"commerce\" + 0.005*\"republican\" + 0.005*\"25%\" + 0.005*\"defense\"')\n",
      "(2, '0.006*\"wrong?\" + 0.006*\"play\" + 0.005*\"actually\" + 0.005*\"zte\" + 0.005*\"ban\" + 0.005*\"u\" + 0.005*\"department\" + 0.005*\"deal\" + 0.005*\"company\" + 0.005*\"lawmaker\" + 0.005*\"billion\" + 0.005*\"bill\" + 0.005*\"senate\" + 0.005*\"said\" + 0.005*\"first\" + 0.005*\"resumed\" + 0.005*\"last\" + 0.005*\"cornyn\" + 0.005*\"trump\" + 0.005*\"chinese\" + 0.005*\"president\" + 0.005*\"company\\'s\" + 0.005*\"measure\" + 0.005*\"25%\" + 0.005*\"want\" + 0.005*\"trading\" + 0.005*\"lost\" + 0.005*\"share\" + 0.005*\"chip\" + 0.005*\"defense\"')\n",
      "(3, '0.006*\"wrong?\" + 0.006*\"actually\" + 0.006*\"play\" + 0.005*\"zte\" + 0.005*\"u\" + 0.005*\"billion\" + 0.005*\"ban\" + 0.005*\"deal\" + 0.005*\"first\" + 0.005*\"department\" + 0.005*\"said\" + 0.005*\"company\" + 0.005*\"president\" + 0.005*\"senate\" + 0.005*\"bill\" + 0.005*\"lost\" + 0.005*\"trump\" + 0.005*\"still\" + 0.005*\"chinese\" + 0.005*\"company\\'s\" + 0.005*\"resumed\" + 0.005*\"last\" + 0.005*\"measure\" + 0.005*\"cornyn\" + 0.005*\"share\" + 0.005*\"25%\" + 0.005*\"kong-listed\" + 0.005*\"want\" + 0.005*\"korea.\" + 0.005*\"hong\"')\n",
      "(4, '0.023*\"zte\" + 0.017*\"ban\" + 0.015*\"u\" + 0.013*\"deal\" + 0.012*\"said\" + 0.012*\"billion\" + 0.012*\"trump\" + 0.011*\"chinese\" + 0.011*\"department\" + 0.010*\"hong\" + 0.010*\"senate\" + 0.009*\"lawmaker\" + 0.009*\"bill\" + 0.008*\"company\" + 0.008*\"last\" + 0.008*\"president\" + 0.008*\"measure\" + 0.008*\"resumed\" + 0.007*\"cornyn\" + 0.007*\"trading\" + 0.007*\"still\" + 0.007*\"first\" + 0.007*\"lost\" + 0.007*\"want\" + 0.007*\"share\" + 0.007*\"company\\'s\" + 0.006*\"defense\" + 0.006*\"25%\" + 0.006*\"say,\"\" + 0.006*\"see\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=5, num_words=30):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\song\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis.gensim_models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary,mds='mmds')\n",
    "pyLDAvis.save_html(vis, './vis.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04c657934001f8e33ca103ce389611c41b8a1110451583a3586fe6f67a3e7753"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}